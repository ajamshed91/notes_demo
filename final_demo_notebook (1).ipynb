{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install sec_api"
      ],
      "metadata": {
        "id": "Jb8tBWAav83u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "W4hWkqeMv1Vi",
        "outputId": "5782c3e4-244f-4311-9b18-cbac7cbc248b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sec_api'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-990505857>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msec_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExtractorApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sec_api'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "SEC Filing Notes Parser\n",
        "This script extracts notes 2, 3, 4, and the last note from SEC filings and processes their readability.\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "from sec_api import ExtractorApi\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import os\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import asyncio\n",
        "from typing import List, Tuple, Dict\n",
        "from openai import AsyncOpenAI\n",
        "\n",
        "async def asynchronous_llm(\n",
        "    messages,\n",
        "    temperature=0.7,\n",
        "    model=None,\n",
        "    provider=\"openai\",\n",
        "    verbose=False,\n",
        "    max_tokens=2048,\n",
        "    **kwargs,\n",
        "):\n",
        "    \"\"\"\n",
        "    Asynchronous function to interact with different LLM providers.\n",
        "\n",
        "    Returns:\n",
        "    - LLM response (either full response object or content, depending on 'verbose')\n",
        "    \"\"\"\n",
        "\n",
        "    DEFAULT_MODELS = {\n",
        "        \"openai\": \"gpt-4o-mini\",\n",
        "    }\n",
        "\n",
        "    if model is None:\n",
        "        model = DEFAULT_MODELS.get(provider, DEFAULT_MODELS[\"openai\"])\n",
        "\n",
        "    if provider == \"openai\":\n",
        "        try:\n",
        "            openai_client = AsyncOpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "            openai_object = {\n",
        "                \"model\": model,\n",
        "                \"messages\": messages,\n",
        "                \"temperature\": temperature,\n",
        "                \"max_tokens\": max_tokens,\n",
        "            }\n",
        "\n",
        "            openai_object.update(kwargs)\n",
        "\n",
        "            response = await openai_client.chat.completions.create(**openai_object)\n",
        "\n",
        "            if not verbose:\n",
        "                return response.choices[0].message.content\n",
        "            return response\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            return None\n",
        "\n",
        "# Placeholder for the async function you'll implement\n",
        "async def get_readability_score(text: str) -> float:\n",
        "    \"\"\"Placeholder for the async function that will calculate readability scores\"\"\"\n",
        "    # To be implemented\n",
        "    system_prompt = \"\"\"Imagine you're an average individual investor. Please read the following excerpts from 10-K filings and rate each one on a scale from 1 to 5 based on how easy it is to understand.\n",
        "\n",
        "1 = Very easy to understand\n",
        "5 = Very difficult to understand or process\n",
        "\n",
        "Do not output anything else, just the score.\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"note: {text}\"\"\"\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    res = await asynchronous_llm(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=chat,\n",
        "        temperature=0.2\n",
        "    )\n",
        "\n",
        "    return res\n",
        "\n",
        "def get_cache_filename(url):\n",
        "    \"\"\"Generate a cache filename from URL\"\"\"\n",
        "    # Create a hash of the URL to use as filename\n",
        "    url_hash = hashlib.md5(url.encode()).hexdigest()\n",
        "    return f\"cache_{url_hash}.html\"\n",
        "\n",
        "def get_html_content(url, section_num, api_key, cache_dir=\"html_cache\"):\n",
        "    \"\"\"Get HTML content either from cache or SEC API\"\"\"\n",
        "    # Create cache directory if it doesn't exist\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    cache_file = os.path.join(cache_dir, get_cache_filename(url))\n",
        "\n",
        "    # Try to read from cache first\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"Reading from cache: {cache_file}\")\n",
        "        with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "\n",
        "    # If not in cache, fetch from API\n",
        "    print(f\"Fetching from SEC API: {url}\")\n",
        "    extractorApi = ExtractorApi(api_key)\n",
        "    html = extractorApi.get_section(url, str(section_num), \"html\")\n",
        "\n",
        "    # Save to cache if content was received\n",
        "    if html:\n",
        "        print(f\"Saving to cache: {cache_file}\")\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            f.write(html)\n",
        "\n",
        "    return html\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by removing extra whitespace and normalizing spaces\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    return text\n",
        "\n",
        "def table_to_markdown(table):\n",
        "    \"\"\"Convert HTML table to markdown format\"\"\"\n",
        "    rows = table.find_all('tr')\n",
        "    if not rows:\n",
        "        return \"\"\n",
        "\n",
        "    markdown = []\n",
        "\n",
        "    # Process header row\n",
        "    header = rows[0]\n",
        "    header_cols = []\n",
        "    for th in header.find_all(['th', 'td']):\n",
        "        text = clean_text(th.get_text())\n",
        "        header_cols.append(text if text else \" \")\n",
        "\n",
        "    if header_cols:\n",
        "        markdown.append(\"| \" + \" | \".join(header_cols) + \" |\")\n",
        "        markdown.append(\"|-\" * len(header_cols) + \"|\")\n",
        "\n",
        "    # Process data rows\n",
        "    for row in rows[1:]:\n",
        "        cols = []\n",
        "        for td in row.find_all(['td', 'th']):\n",
        "            text = clean_text(td.get_text())\n",
        "            cols.append(text if text else \" \")\n",
        "        if cols:\n",
        "            markdown.append(\"| \" + \" | \".join(cols) + \" |\")\n",
        "\n",
        "    return \"\\n\".join(markdown) + \"\\n\\n\"\n",
        "\n",
        "def extract_notes(html_content) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Extract notes 2, 3, 4, and the last note from the HTML content\"\"\"\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "    all_notes = []\n",
        "    current_note = None\n",
        "    current_content = []\n",
        "\n",
        "    # Look for note headers and their content\n",
        "    for element in soup.find_all(['div', 'p', 'table']):\n",
        "        text = clean_text(element.get_text())\n",
        "\n",
        "        # Check if this is a note header (handle both number and letter formats)\n",
        "        note_match = re.match(r'Note\\s+(\\d+|\\w+)\\s*[–-]\\s*(.+)', text, re.IGNORECASE)\n",
        "        if note_match:\n",
        "            # If we were processing a previous note, save it\n",
        "            if current_note and current_content:\n",
        "                all_notes.append((current_note, ''.join(current_content)))\n",
        "\n",
        "            # Start new note\n",
        "            current_note = text\n",
        "            current_content = []\n",
        "            continue\n",
        "\n",
        "        # If we're in a note, collect its content\n",
        "        if current_note:\n",
        "            if element.name == 'table':\n",
        "                # Convert table to markdown\n",
        "                table_md = table_to_markdown(element)\n",
        "                if table_md:\n",
        "                    current_content.append(table_md)\n",
        "            else:\n",
        "                if text:\n",
        "                    current_content.append(text + \"\\n\\n\")\n",
        "\n",
        "    # Don't forget to add the last note\n",
        "    if current_note and current_content:\n",
        "        all_notes.append((current_note, ''.join(current_content)))\n",
        "\n",
        "    # Filter for notes 2, 3, 4, and the last note\n",
        "    selected_notes = []\n",
        "    for i, (title, content) in enumerate(all_notes):\n",
        "        note_num_match = re.match(r'Note\\s+(\\d+)', title)\n",
        "        if note_num_match:\n",
        "            note_num = int(note_num_match.group(1))\n",
        "            if note_num in [2, 3, 4] or i == len(all_notes) - 1:\n",
        "                selected_notes.append((title, content))\n",
        "\n",
        "    return selected_notes\n",
        "\n",
        "async def process_notes_with_scores(notes: List[Tuple[str, str]]) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"Process all notes in parallel and get their readability scores\"\"\"\n",
        "    tasks = []\n",
        "    for title, content in notes:\n",
        "        task = asyncio.create_task(get_readability_score(content))\n",
        "        tasks.append((title, content, task))\n",
        "\n",
        "    # Wait for all scoring tasks to complete\n",
        "    results = []\n",
        "    for title, content, task in tasks:\n",
        "        score = await task\n",
        "        results.append((title, content, score))\n",
        "\n",
        "    return results\n",
        "\n",
        "async def process_filing(url: str, section_num: str, api_key: str) -> List[Tuple[str, str, float]]:\n",
        "    \"\"\"Process a specific filing and return notes with their readability scores\"\"\"\n",
        "    try:\n",
        "        # Get HTML content (either from cache or API)\n",
        "        html = get_html_content(url, section_num, api_key)\n",
        "\n",
        "        if not html:\n",
        "            raise ValueError(f\"No content found in section {section_num}\")\n",
        "\n",
        "        # Extract notes from HTML\n",
        "        notes = extract_notes(html)\n",
        "\n",
        "        if not notes:\n",
        "            print(f\"Warning: No notes found in section {section_num}\")\n",
        "            return []\n",
        "\n",
        "        print(f\"Found {len(notes)} notes to process\")\n",
        "        for title, _ in notes:\n",
        "            print(f\"  - {title}\")\n",
        "\n",
        "        # Process notes and get readability scores\n",
        "        return await process_notes_with_scores(notes)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing filing: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def save_results(company: str, results: List[Tuple[str, str, float]], output_dir: str = \"results\"):\n",
        "    \"\"\"Save results to a structured text file\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    output_file = os.path.join(output_dir, f\"{company}_results.txt\")\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        for title, content, score in results:\n",
        "            # Write in CSV-like format: note_title, content_preview, score\n",
        "            content_preview = content[:200].replace('\\n', ' ').replace(',', ' ')  # Basic content preview\n",
        "            f.write(f\"{title},{content_preview}...,{score}\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FUgWiBxlJ6xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def main():\n",
        "    # SEC API Key\n",
        "    SEC_API_KEY = \"_\"\n",
        "\n",
        "    # List of SEC filings to process\n",
        "    filings = [\n",
        "        # Apple Inc. 10-K 2018\n",
        "        \"https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/a10-k20189292018.htm\",\n",
        "        # Microsoft 10-K 2023\n",
        "        \"https://www.sec.gov/Archives/edgar/data/789019/000156459023029823/msft-10k_20230630.htm\",\n",
        "        # Tesla 10-K 2023\n",
        "        \"https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm\"\n",
        "    ]\n",
        "\n",
        "    # Process all filings in parallel\n",
        "    tasks = []\n",
        "    for url in filings:\n",
        "        company = url.split('/')[-1].split('-')[0].split('.')[0]\n",
        "        print(f\"\\nProcessing {company}...\")\n",
        "\n",
        "        task = asyncio.create_task(process_filing(url, \"8\", SEC_API_KEY))\n",
        "        tasks.append((company, task))\n",
        "\n",
        "    # Wait for all filings to be processed\n",
        "    for company, task in tasks:\n",
        "        try:\n",
        "            results = await task\n",
        "            if results:\n",
        "                save_results(company, results)\n",
        "                print(f\"Results saved for {company}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {company}: {str(e)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(main())"
      ],
      "metadata": {
        "id": "lDI0E6Xtv56I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}